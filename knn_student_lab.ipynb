{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUC7NVBu4CDf"
      },
      "source": [
        "# kNN & Distance-Based Learning — Student Lab\n",
        "\n",
        "Complete all TODOs. Focus: vectorization, scaling, and failure modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_SJMqD5J4CDg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Ky9AbE4CDg"
      },
      "source": [
        "## Section 0 — Synthetic dataset generator\n",
        "We’ll generate Gaussian blobs with controllable dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHhWIOEr4CDh",
        "outputId": "f570cc8d-fc8b-496c-875e-3c7089942bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: shapes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((400, 5), (200, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def make_blobs(n_train=400, n_test=200, d=2, sep=2.5):\n",
        "    # two Gaussian blobs\n",
        "    mu0 = np.zeros(d)\n",
        "    mu1 = np.zeros(d); mu1[0] = sep\n",
        "    X0 = rng.standard_normal((n_train//2, d)) + mu0\n",
        "    X1 = rng.standard_normal((n_train - n_train//2, d)) + mu1\n",
        "    X_train = np.vstack([X0, X1])\n",
        "    y_train = np.array([0]*len(X0) + [1]*len(X1))\n",
        "    perm = rng.permutation(n_train)\n",
        "    X_train, y_train = X_train[perm], y_train[perm]\n",
        "\n",
        "    T0 = rng.standard_normal((n_test//2, d)) + mu0\n",
        "    T1 = rng.standard_normal((n_test - n_test//2, d)) + mu1\n",
        "    X_test = np.vstack([T0, T1])\n",
        "    y_test = np.array([0]*len(T0) + [1]*len(T1))\n",
        "    perm = rng.permutation(n_test)\n",
        "    X_test, y_test = X_test[perm], y_test[perm]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_test, y_test = make_blobs(d=5)\n",
        "check('shapes', X_train.shape[0] == y_train.shape[0] and X_test.shape[0] == y_test.shape[0])\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSitv6fy4CDh"
      },
      "source": [
        "## Section 1 — Vectorized distances\n",
        "\n",
        "### Task 1.1: L2 distance matrix\n",
        "Compute D where D[i,j] = ||X_test[i] - X_train[j]||_2.\n",
        "\n",
        "# HINT:\n",
        "- Use expansion: ||a-b||^2 = ||a||^2 + ||b||^2 - 2 a·b\n",
        "- Avoid building (m,n,d) broadcast tensor for large sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj8ZK6X64CDh",
        "outputId": "076acaaf-8cb9-42f6-c60e-3e4d9c8b846e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: D_shape\n",
            "OK: D_nonneg\n"
          ]
        }
      ],
      "source": [
        "def l2_distances(X_test, X_train):\n",
        "    # TODO: return (m,n) matrix\n",
        "    test_2 = np.sum(X_test * X_test, axis = 1, keepdims= True)\n",
        "    train_2 = np.sum(X_train * X_train, axis = 1, keepdims= True).T\n",
        "    test_train_2 = 2*(X_test @ X_train.T)\n",
        "    distance = test_2 + train_2 - test_train_2\n",
        "    distance2 = np.maximum(distance, 0.0)\n",
        "\n",
        "    return np.sqrt(distance2)\n",
        "\n",
        "D = l2_distances(X_test[:10], X_train[:20])\n",
        "check('D_shape', D.shape == (10, 20))\n",
        "check('D_nonneg', np.all(D >= -1e-9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2BvQ1n04CDh"
      },
      "source": [
        "### Task 1.2: L1 distances (optional)\n",
        "Compute L1 distance matrix (can use broadcasting here since we’ll keep sizes small).\n",
        "\n",
        "**Interview Angle:** When might L1 beat L2?\n",
        "\n",
        "**Answer:** L1 is often better when data has outliers or is high-dimensional and sparse, because it’s less dominated by a few large differences than L2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKxp1Z_c4CDh",
        "outputId": "14e8efb8-4adc-4208-c3e0-ada951667201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: D1_shape\n"
          ]
        }
      ],
      "source": [
        "def l1_distances(X_test, X_train):\n",
        "    # TODO\n",
        "    return np.sum(np.abs(X_test[:, None, :] - X_train[None, :, :]), axis = 2)\n",
        "\n",
        "D1 = l1_distances(X_test[:5], X_train[:7])\n",
        "check('D1_shape', D1.shape == (5, 7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_uRO6nR4CDi"
      },
      "source": [
        "## Section 2 — kNN classifier\n",
        "\n",
        "### Task 2.1: Predict with kNN\n",
        "\n",
        "# HINT:\n",
        "- compute distances\n",
        "- argsort distances to find k nearest\n",
        "- majority vote\n",
        "\n",
        "**FAANG gotcha:** define tie-breaking deterministically (e.g., pick smallest label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdrHkZxh4CDi",
        "outputId": "797831ed-b9ea-420e-e448-634f629e3c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: yhat_shape\n",
            "OK: labels\n"
          ]
        }
      ],
      "source": [
        "def knn_predict(X_train, y_train, X_test, k=3):\n",
        "    # TODO\n",
        "    d = l2_distances(X_test, X_train)\n",
        "    nn_idx = np.argsort(d, axis = 1)[:, :k]\n",
        "    nn_label = y_train[nn_idx]\n",
        "    votes = nn_label.sum(axis = 1)\n",
        "    return (votes * 2 > k).astype(int)\n",
        "\n",
        "yhat = knn_predict(X_train, y_train, X_test[:20], k=3)\n",
        "check('yhat_shape', yhat.shape == (20,))\n",
        "check('labels', set(np.unique(yhat)).issubset({0,1}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEgEKofk4CDi"
      },
      "source": [
        "### Task 2.2: Evaluate over k\n",
        "Compute accuracy for k in [1,3,5,9,15].\n",
        "\n",
        "**Checkpoint:** Why does increasing k usually increase bias and reduce variance?\n",
        "\n",
        "**Answer:**\n",
        "- With small k (like k = 1):\n",
        "\t- The prediction depends on very few points\n",
        "\t- A single noisy point can flip the prediction\n",
        "\t- Model changes a lot if data changes slightly\n",
        "  - ***Low bias, high variance***\n",
        "- With large k:\n",
        "\t- The prediction is an average over many points\n",
        "\t- Individual noisy points matter less\n",
        "\t- Predictions become smoother and more stable\n",
        "  - ***Higher bias, lower variance***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JShrMad14CDj",
        "outputId": "824d0734-f5ea-40fc-f798-c038385755fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k 1 train_acc 1.0 test_acc 0.825\n",
            "k 3 train_acc 0.9325 test_acc 0.87\n",
            "k 5 train_acc 0.905 test_acc 0.885\n",
            "k 9 train_acc 0.89 test_acc 0.88\n",
            "k 15 train_acc 0.885 test_acc 0.89\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y, yhat):\n",
        "    return float(np.mean(y == yhat))\n",
        "\n",
        "ks = [1,3,5,9,15]\n",
        "for k in ks:\n",
        "    yhat_tr = knn_predict(X_train, y_train, X_train, k=k)\n",
        "    yhat_te = knn_predict(X_train, y_train, X_test, k=k)\n",
        "    print('k', k, 'train_acc', accuracy(y_train, yhat_tr), 'test_acc', accuracy(y_test, yhat_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3zRBuyd4CDj"
      },
      "source": [
        "## Section 3 — Curse of dimensionality\n",
        "\n",
        "### Task 3.1: Distance concentration\n",
        "For increasing dimension d, compute ratio min_dist/max_dist for random points and show it approaches 1.\n",
        "\n",
        "# HINT:\n",
        "- sample n points in d dims\n",
        "- compute pairwise distances from one reference point\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q44xz7H4CDj",
        "outputId": "c325aa6d-82ce-4885-9d5f-dabe0eca3278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d 2 min/max 0.006214278718774088\n",
            "d 5 min/max 0.10968424369044452\n",
            "d 10 min/max 0.28327247598849514\n",
            "d 50 min/max 0.4947050997185768\n",
            "d 100 min/max 0.6569330655910577\n"
          ]
        }
      ],
      "source": [
        "def concentration_ratio(n=2000, dims=(2,5,10,50,100)):\n",
        "    ratios = []\n",
        "    for d in dims:\n",
        "        X = rng.standard_normal((n, d))\n",
        "        ref = X[0:1]\n",
        "        D = l2_distances(ref, X)[0, 1:]\n",
        "        ratios.append((d, float(D.min() / D.max())))\n",
        "    return ratios\n",
        "\n",
        "ratios = concentration_ratio()\n",
        "for d, r in ratios:\n",
        "    print('d', d, 'min/max', r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsnIPRa24CDj"
      },
      "source": [
        "## Section 4 — Feature scaling sensitivity\n",
        "\n",
        "### Task 4.1: Break kNN with scaling\n",
        "Create a dataset where one feature has huge scale and show accuracy drops. Then fix with standardization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ6xPuYK4CDj",
        "outputId": "a4c61a97-1d29-46c3-c6ef-2ae3332cabbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc_bad 0.555\n",
            "acc_std 0.85\n",
            "OK: improves\n"
          ]
        }
      ],
      "source": [
        "Xtr, ytr, Xte, yte = make_blobs(d=2)\n",
        "# blow up feature 1 scale\n",
        "Xtr_bad = Xtr.copy(); Xte_bad = Xte.copy()\n",
        "Xtr_bad[:, 1] *= 1000\n",
        "Xte_bad[:, 1] *= 1000\n",
        "\n",
        "yhat_bad = knn_predict(Xtr_bad, ytr, Xte_bad, k=5)\n",
        "acc_bad = accuracy(yte, yhat_bad)\n",
        "print('acc_bad', acc_bad)\n",
        "\n",
        "# TODO: standardize using train mean/std and re-evaluate\n",
        "mu = Xtr_bad.mean(axis=0, keepdims=True)\n",
        "sd = Xtr_bad.std(axis=0, keepdims=True)\n",
        "Xtr_std = (Xtr_bad - mu) / sd\n",
        "Xte_std = (Xte_bad - mu) /sd\n",
        "\n",
        "yhat_std = knn_predict(Xtr_std, ytr, Xte_std, k=5)\n",
        "acc_std = accuracy(yte, yhat_std)\n",
        "print('acc_std', acc_std)\n",
        "check('improves', acc_std >= acc_bad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlt0QoQV4CDk"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- k-sweep results shown\n",
        "- concentration experiment done\n",
        "- scaling fix demonstrated\n"
      ]
    }
  ]
}